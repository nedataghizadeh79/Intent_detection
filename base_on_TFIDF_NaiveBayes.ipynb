{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h2 dir=\"rtl\">\n",
        "در این ثسمت میخواهیم که پروژه تشخیص قصد با استفاده از Naive Baise بر روی وکتورهای tf-idf به زبان فارسی پیاده را پیاده کنیم.\n",
        "\n",
        "\n",
        "</h2>"
      ],
      "metadata": {
        "id": "mV_AT4bCjpcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h4 dir=\"rtl\">\n",
        "کتابخانه‌های مورد نیاز برای دیتاست‌ها، ارزیابی‌ها، پردازش متنی به زبان فارسی، مدل‌های متنی، و... برای آموزش و اجرای مدل‌های متنی هستند.\n",
        "</h4>"
      ],
      "metadata": {
        "id": "MPIAFVEdcLeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install -q datasets\n",
        "!pip install -q evaluate\n",
        "!pip install -q --upgrade hazm\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q torch\n",
        "!pip install -q rich[jupyter]\n",
        "!pip install -q datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F02GxjqTRz_Q",
        "outputId": "e0a8d0e0-a3c1-4690-9204-e75077e41620"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 dir=\"rtl\">\n",
        " در کد زیر هم کتابخانه های مورد نظر را فرا خواندیم که در ادامه شرح میدهیم که هر یک چه کار میکنند\n",
        "\n",
        "  \n",
        " numpy  و skearn برای محاسبات علمی و یادگیری ماشین به کار گرفته می‌شوند.\n",
        "\n",
        "\n",
        " از TfidfVectorizer در scikit-learn برای تبدیل متن‌ها به بردارهای TF-IDF استفاده می‌شود. این بردارها ویژگی‌های متن را با توجه به میزان تکرار و مهمیت کلمات در متن محاسبه می‌کنند.\n",
        "\n",
        "\n",
        " train_test_split  برای تقسیم مجموعه داده به دو بخش آموزش و آزمون به نسبت مشخصی به کار می‌رود.\n",
        "\n",
        "\n",
        " classification_report  برای ارزیابی عملکرد مدل بر روی داده‌های آزمون با استفاده از معیارهای دقت، بازیابی، و امتیاز F1 به کار می‌رود.\n",
        "\n",
        "\n",
        "از مدل MultinomialNB در scikit-learn به عنوان مدل یادگیری ماشین برای تشخیص اندازه‌گیری (Intent Detection) استفاده می‌شود. این مدل از روش بیز ساده برای پیش‌بینی دسته‌های مختلف بر اساس بردارهای TF-IDF استفاده می‌کند.\n",
        "\n",
        "\n",
        "load_dataset از کتابخانه Hugging Face Datasets برای بارگذاری دیتاست مورد استفاده در پروژه به کار می‌رود.\n",
        "\n",
        "</h3>"
      ],
      "metadata": {
        "id": "1V1k4x9ycTrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "Y4x4w0TATkjv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXmxg5YhVYQZ",
        "outputId": "dbce2b3b-8f53-4955-a724-cce5223551ca"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "F457Rrx3VbEW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 dir=\"rtl\">\n",
        "\n",
        "\n",
        " از دستور load_dataset(\"AmazonScience/massive\", 'fa-IR')  برای بارگیری مجموعه داده استفاده می‌کنیم.\n",
        "\n",
        "\n",
        " dataset = dataset.rename_column(\"utt\", \"text\") در این قسمت، نام ستون \"utt\" (متن داده) به \"text\" تغییر میدیم.\n",
        "\n",
        "\n",
        " dataset = dataset.rename_column(\"scenario\", \"label\")`  نام ستون \"scenario\" (برچسب‌ها) به \"label\" تغییر میدهیم.  \n",
        "\n",
        "\n",
        "  در قسمت آخر هم داده‌های آموزش و آزمون از مجموعه داده به دست می‌آید.\n",
        "</h3>"
      ],
      "metadata": {
        "id": "e8-G_hoCdmBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"AmazonScience/massive\", 'fa-IR')\n",
        "dataset = dataset.rename_column(\"utt\", \"text\")\n",
        "dataset = dataset.rename_column(\"scenario\", \"label\")\n",
        "\n",
        "# Access the text and label columns based on the keys\n",
        "train_texts = dataset['train']['text']\n",
        "train_labels = dataset['train']['label']\n",
        "test_texts = dataset['test']['text']\n",
        "test_labels = dataset['test']['label']\n"
      ],
      "metadata": {
        "id": "wdE9dOdFRGqN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 dir=\"rtl\">\n",
        " در ادامه میخواهیم که تبدیل متن‌ها به بردارهای عددی را در جهت استفاده در مدل‌های یادگیری ماشینی استفاده کنیم.\n",
        "\n",
        "\n",
        "\n",
        "در ابتدا یک شیء از کلاس TF-IDF Vectorizer ایجاد می‌کنیم.\n",
        "max_features که بیانگر تعداد حداکثر کلماتی که به عنوان ویژگی‌ها در نظر گرفته می‌شوند را ۵۰۰۰ قرار میدهیم.\n",
        "encoding و کدگذاری متن‌ها دا به صورت 'utf-8' مشخص میکنیم.\n",
        "سپس، با استفاده از fit_transform، متن‌های آموزش (train_texts) به بردارهای TF-IDF تبدیل می‌کینم و در train_tfidf ذخیره می‌کینم. این بردارها شامل اطلاعات مرتبط با فراوانی واژه‌ها در متن‌های آموزش هستند.\n",
        "\n",
        "\n",
        "\n",
        "</h3>"
      ],
      "metadata": {
        "id": "6T2DYWREfBCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words=None, encoding='utf-8')\n",
        "train_tfidf = tfidf_vectorizer.fit_transform(train_texts)\n",
        "test_tfidf = tfidf_vectorizer.transform(test_texts)"
      ],
      "metadata": {
        "id": "WpO2CRgKfMjZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 dir=\"rtl\">\n",
        "در ادامه میخواهیم تبدیل برچسب‌های دسته‌بندی شده به اعداد صحیح را انجام بدیم.\n",
        "\n",
        "\n",
        " یک نمونه از کلاس  LabelEncoder ایجاد می‌کنیم. سپس fit_transform برای آموزش و تبدیل برچسب‌های آموزشی (train_labels) به اعداد صحیح استفاده می‌شود. این عمل با تبدیل هر برچسب به یک عدد منحصر به فرد انجام می‌شود.\n",
        " transform همچنین برای تبدیل برچسب‌های تستی (test_labels) به اعداد صحیح استفاده می‌شود.\n",
        "\n",
        "\n",
        "\n",
        "</h3>"
      ],
      "metadata": {
        "id": "aLup_96lfNSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
        "test_labels_encoded = label_encoder.transform(test_labels)"
      ],
      "metadata": {
        "id": "qM2oVQDYfPlW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 dir=\"rtl\">\n",
        "در ادامه از الگوریتم MultinomialNB استفاده می‌کنیم که برای مسائل طبقه‌بندی چند کلاسه معمولاً مناسب است.\n",
        "\n",
        " naive_bayes_classifier = MultinomialNB() : ایجاد یک نمونه از مدل کلاس‌بندی بیزی چند جمله‌ای ایجاد مکنیم.\n",
        "سپس مدل  را با استفاده از داده‌های آموزش train_tfidf و برچسب‌های متناظر با آنها   train_labels_encoded اوکی میکنیم که train_tfidf  حاوی بردارهای TF-IDF مربوط به متن‌های آموزش است و  train_labels_encoded حاوی برچسب‌های عددی متناظر با کلاس‌های مختلف مسئله است.\n",
        "\n",
        "\n",
        "\n",
        "</h3>"
      ],
      "metadata": {
        "id": "szfiICNofY8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Naive Bayes classifier\n",
        "naive_bayes_classifier = MultinomialNB()\n",
        "naive_bayes_classifier.fit(train_tfidf, train_labels_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "O2-5u1FbfSRI",
        "outputId": "93bf5577-b634-4db5-af42-7a62e4363025"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 dir=\"rtl\">\n",
        "حال به سراغ  پیش‌بینی برچسب‌ها بر روی مجموعه آزمون و  بازگرداندن برچسب‌های پیش‌بینی شده به شکل اصلی میرویم.\n",
        "\n",
        "در ابتدا مدل ماشین یادگیری Naive Bayes برای پیش‌بینی برچسب‌های کلاس‌ها بر روی داده‌های تست استفاده می‌شود. به عبارت دیگر، ما داریم پیشبینی میکنیم که هر جمله تست به کدام کلاس از کلاس‌ها مپ می‌شود. سپس برچسب‌هایی که توسط مدل پیش‌بینی شده‌اند (به صورت اعداد) به تبدیل به برچسب‌های اصلی (رشته‌ها) می‌پردازیم.\n",
        "\n",
        "\n",
        "</h3>"
      ],
      "metadata": {
        "id": "60x58oubfaxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict labels on the test set\n",
        "predicted_labels = naive_bayes_classifier.predict(test_tfidf)\n",
        "\n",
        "# Decode labels\n",
        "predicted_labels_decoded = label_encoder.inverse_transform(predicted_labels)"
      ],
      "metadata": {
        "id": "GSf1Swt3fTLS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 dir=\"rtl\">\n",
        "\n",
        "\n",
        "\n",
        "کد ابتدا از ماژول sklearn.metrics تابع classification_report را فراخوانی می‌کند. سپس این تابع بر اساس پیش‌بینی‌های مدل predicted_labels_decoded  و برچسب‌های واقعی test_labels ، یک گزارش ارزیابی ایجاد می‌کند.  \n",
        "\n",
        "\n",
        "\n",
        "</h3>"
      ],
      "metadata": {
        "id": "XGftClFPfdl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the classifier\n",
        "classification_rep = classification_report(test_labels, predicted_labels_decoded, labels=np.unique(predicted_labels_decoded))\n",
        "print(classification_rep)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hsmo4U9cYfpW",
        "outputId": "81c4cc46-0d89-4159-bd95-c8923d89702c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.52      0.68       106\n",
            "           1       0.94      0.89      0.91       124\n",
            "           2       0.49      0.99      0.65       402\n",
            "           3       0.79      0.99      0.88       387\n",
            "           4       0.97      0.74      0.84       124\n",
            "           5       1.00      0.53      0.70       103\n",
            "           6       0.94      0.53      0.68        94\n",
            "           7       0.81      0.94      0.87       271\n",
            "           8       0.96      0.94      0.95       220\n",
            "           9       0.77      0.23      0.35       189\n",
            "          10       1.00      0.52      0.68        62\n",
            "          11       0.88      0.53      0.66       142\n",
            "          12       0.75      0.85      0.79       288\n",
            "          13       0.96      0.35      0.51        72\n",
            "          14       1.00      0.61      0.76        57\n",
            "          15       1.00      0.31      0.47        81\n",
            "          16       0.98      0.48      0.64        96\n",
            "          17       0.95      0.85      0.89       156\n",
            "\n",
            "    accuracy                           0.76      2974\n",
            "   macro avg       0.90      0.65      0.72      2974\n",
            "weighted avg       0.83      0.76      0.75      2974\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 dir=\"rtl\">\n",
        "\n",
        "\n",
        "در ادامه هم جمله ورودی میدیم و خروجی را مشاهده میکنیم.\n",
        "منتها خروجی ما به صودت عدد است به خاطر نوع دیتاست مان.\n",
        "\n",
        "</h3>"
      ],
      "metadata": {
        "id": "JqSejAwzivAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of predicting the intent of a new sentence\n",
        "new_sentence = \"گوشی هوشمند من نیازمند ماژول است\"\n",
        "new_sentence_tfidf = tfidf_vectorizer.transform([new_sentence])\n",
        "predicted_intent = naive_bayes_classifier.predict(new_sentence_tfidf)\n",
        "predicted_intent_decoded = label_encoder.inverse_transform(predicted_intent)\n",
        "print(f\"Predicted intent for '{new_sentence}': {predicted_intent_decoded[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DurtC4G3Yx_7",
        "outputId": "aa40daae-f819-42aa-d09e-fce4bdde47eb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted intent for 'گوشی هوشمند من نیازمند ماژول است': 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 dir=\"rtl\">\n",
        "\n",
        "\n",
        "در ادامه یک تابع به نام map_number_to_word ایجاد می‌کنیم که عددی و یک آرایه از کلمات به عنوان ورودی می‌گیرد. سپس اگر عدد در محدوده صحیح واقعی بین ۰ و تعداد کلمات در آرایه باشد، تابع کلمه متناظر با این عدد را برمی‌گرداند.\n",
        "\n",
        "\n",
        "</h3>"
      ],
      "metadata": {
        "id": "G2002hbzixco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_number_to_word(number, word_array):\n",
        "    if 0 <= int(number) < len(word_array):\n",
        "        return word_array[int(number)]\n",
        "    else:\n",
        "        return \"Number out of range\"\n",
        "\n",
        "\n",
        "word_array = [\n",
        "    'social', 'transport', 'calendar', 'play', 'news', 'datetime',\n",
        "    'recommendation', 'email', 'iot', 'general', 'audio', 'lists',\n",
        "    'qa', 'cooking', 'takeaway', 'music', 'alarm', 'weather'\n",
        "]\n"
      ],
      "metadata": {
        "id": "mgI15JMqZ8qX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 dir=\"rtl\">\n",
        "\n",
        "در نهایت خروجی مان به صودت موضوعی مشخص میشود\n",
        "\n",
        "\n",
        "</h3>"
      ],
      "metadata": {
        "id": "ZfF-Q0Rrjhes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = map_number_to_word(predicted_intent_decoded[0], word_array)\n",
        "print(f\"Number {int(predicted_intent_decoded[0])} means : '{result}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kdF6ULYjQu6",
        "outputId": "682ed8c3-70ec-4928-d9d4-a070042a7c0d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number 8 means : 'iot'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 dir=\"rtl\">\n",
        "مقایسه:\n",
        "\n",
        "\n",
        "\n",
        "  عملکرد (Performance)\n",
        "   - روش TF-IDF و Naive Bayes: این روش برای مجموعه داده‌های کوچک تا متوسط ​​بسیار سریع و دقیق عمل می‌کند. اگر تعداد برچسب‌ها و تعداد واژه‌های مجموعه داده کم باشد، این روش به عنوان یک گزینه مناسب مطرح می‌شود.\n",
        "   - روش مبتنی بر ترنسفورمر: ترنسفورمرها، به ویژه مدل‌های بزرگتر، برای مجموعه داده‌های بزرگ و پیچیده عملکرد بهتری دارند. آن‌ها می‌توانند از اطلاعات متنی بهتری برای دسته‌بندی استفاده کنند و از انتقال یادگیری نیز بهره‌مند شوند.\n",
        "\n",
        " کارایی (Efficiency):\n",
        "   - روش TF-IDF و Naive Bayes: این روش در مقایسه با ترنسفورمرها کمترین نیاز به منابع محاسباتی دارد. بنابراین، از نظر کارایی و سرعت، این روش ممکن است بهتر باشد.\n",
        "   - روش مبتنی بر ترنسفورمر: ترنسفورمرها نیاز به مدل‌های بزرگ و محاسبات بیشتر دارند. این ممکن است منابع بیشتری را مصرف کند و زمان زیادی برای آموزش و پیش‌بینی‌ها در مقایسه با TF-IDF و Naive Bayes ببرد.\n",
        "\n",
        " حافظه (Memory):\n",
        "   - روش TF-IDF و Naive Bayes: این روش به حافظه کمتری نیاز دارد. حجم ماتریس TF-IDF به تعداد واژه‌های موجود در متن و تعداد نمونه‌ها بستگی دارد.\n",
        "   - روش مبتنی بر ترنسفورمر: ترنسفورمرها به حافظه بیشتری نیاز دارند، به ویژه مدل‌های بزرگ. حجم مدل‌های ترنسفورمر معمولاً بزرگتر است و ممکن است به حافظه بیشتری نیاز داشته باشد.\n",
        "\n",
        " امکانات (Capabilities):\n",
        "   - روش TF-IDF و Naive Bayes: این روش برای مجموعه داده‌های کم تنوع، محدود به واژگان موجود در متن‌هاست و نیاز به پیش‌پردازش دقیق دارد. نیاز به تنظیمات دقیق‌تر است.\n",
        "   - روش مبتنی بر ترنسفورمر: ترنسفورمرها به نسبت بهتری می‌توانند با متون گوناگون و معناهای مختلف کار کنند و نیاز به پیش‌پردازش کمتری دارند. آموزش و تطابق بهتری با داده‌ها دارند.\n",
        "\n",
        "</h3>"
      ],
      "metadata": {
        "id": "PUJQSHKUkNJD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 dir=\"rtl\">\n",
        "نتیجه گیری:\n",
        "\n",
        "\n",
        "\n",
        "بنابراین، اگر مجموعه داده   کوچک و ساده است و توانایی پردازش مصرف حافظه و محدودیت منابع محاسباتی را دارید، روش TF-IDF و Naive Bayes ممکن است بهترین گزینه باشد. اگر مجموعه داده   بزرگتر و پیچیده‌تر است و توانایی ایجاد و استفاده از مدل‌های بزرگتر و پیچیده‌تر را دارید، روش مبتنی بر ترنسفورمر ممکن است بهترین انتخاب باشد.\n",
        "\n",
        "\n",
        "\n",
        "</h3>"
      ],
      "metadata": {
        "id": "gra5ggK1mKxD"
      }
    }
  ]
}